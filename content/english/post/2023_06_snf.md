---
author: Martin Trapp
title: Mixtures with Negative Weights
date: 2023-06-15
tags: 
  - Technical Note
math: true
---


It looks like I did not quite manage to stick to my "one post per month" idea.
Nevertheless, here is another more or less technical post.
This time, I will review some work that got me very excited about over the last couple of months.
**Tractable probabilistic models** over continuous domains that can encode **negative** dependencies! WOW!


Before we start, I will set the stage and talk a little about mixture models. 
Mixture models are one of the fundamental approaches in statistics and machine learning.
They have been heavily studied, including various extensions of having hierarchical or nested mixtures or infinite mixtures such as Dirichlet process mixtures and the list goes on. Moreover, there is plenty of theory around mixture models that I will not discuss here.

In essence, a mixture model computes a convex combination of multiple different distributions (e.g., Gaussian distributions) to form a more complicated density function.
A simple example would be a mixture of two components, 
$$
f(x) = w p(x \mid \theta_1) + (1 - w) p(x \mid \theta_2)
$$
which, in case we talk about Gaussian mixtures or mixtures of Gaussians, could look like:
$$
f(x) = 0.3 \mathcal{N}(x \mid 0, 1) + 0.7 \mathcal{N}(x \mid 3, 0.5)
$$
corresponding to a density function that computes a weighted sum of density functions of Gaussian distributions.

An obvious disadvantage of mixtures is that they *additively* combine density functions of simpler distributions to build complex functions. Meaning, you cannot have negative parameters and thus a mixture model cannot "subtract" density in regions where we expect low density. Hence, potentially requires a large number of mixing components with small variance to represent interesting density functions. The reason for this is simple, we have to ensure that \\(f(x) \geq 0\\) for every \\(x\\) in the support of the mixing distribution and the "easiest" way to do this is to make sure all parameters are positive.

### Non-negative Linear Models
An interesting recent idea, which originates from work on modeling non-parametric models for non-negative functions [[1]](#1), is the following. 
First, consider a linear model of the following form:
$$
g(x) = \phi(x)^T  \textbf{v} \tag{1}
$$
where \\(\textbf{v}\\) is a vector containing parameters and \\(\phi \colon \mathcal{X} \to \mathcal{H}\\) is a mapping function (feature map) that maps each input/datum \\(x \in \mathcal{X}\\) to some element in a Hilbert space \\(\mathcal{H}\\), for example, into \\(\mathcal{H} = \mathbb{R}^D\\). So far so good.

#### Why is this linear model a good thing?
There are multiple good aspects of the model above, (i) it is rather simple and amendable to theoretical analysis, (ii) the learning problem of finding $\textbf{v}$ is convex (nice), (iii) we can easily differentiate the function, and (iv) the model above is a universal function approximator. The last point means that we can approximate any continuous function with \\(g(\cdot)\\) under some conditions for \\(\phi(\cdot)\\). Moreover, in the non-parametric setting, meaning if we project into some infinite-dimensional vector space or said differently if \\(\mathcal{H}\\) is a function space, then we can still retain a finite form of \\(g(\cdot)\\) through the famous representer theorem given as:
$$
g(x) = \sum^n_{i=1} \alpha_i k(x, x_i) \tag{2}
$$
for some kernel function \\(k(\cdot, \cdot)\\) that is associated with \\(\phi(x)\\), i.e., \\(k(x,x') = \phi(x)\phi(x')\\). 
If we want to ensure positivity of \\(g(\cdot)\\) we need to ensure that \\(\\alpha_i \geq 0\\) for every $i$.
If you have never heard of the representer theorem then this went probably over your head. Don't worry, everything that follows will be sufficiently high level. If you want to learn more about the representer theorem I recommend reading [[3]](#3).

#### Non-negative linear models with negative parameters
There are a couple of ways one could try to get negative parameters/weights into Eq. (2) while ensuring that \\(g(x) \geq 0\\) for all \\(x\\).
A particularly nice one has been proposed by [[1]](#1) and extends Eq. (1) in the following way, 
$$
h(x) = \phi(x)^T \textbf{M} \phi(x) = \sum^n_{i,j=1} A_{i,j} k(x, x_i)k(x, x_j) \tag{3}
$$
where \\(\textbf{M}\\) is a linear operator and \\(\textbf{A}\\) is a positive semidefinite matrix. In short what that means is that \\(\textbf{z}^T \textbf{A} \textbf{z} \geq 0\\) for any $\textbf{z}$.
Well, this is exactly what we want. 
Note that I skipped a lot of technical details here, for example, [[1]](#1) shows a representer theorem for this case which establishes the right-hand side of Eq. (3).

Another way to look at Eq. (3) is as a sum of squares or a quadratic form.
A simple example of such a quadratic form would be to just square \\(g(x)\\) in which case \\(g(x)^2 \geq 0\\) for any choice of $\alpha$.

It turns out that, (i) Eq. (3) is again very nicely amendable to theoretical analysis, (ii) the learning problem of finding \\(\textbf{A}\\) is still convex (nice), (iii) we can still easily differentiate the function, (iv) the model above is again a universal function approximator, and (v) **this formulation can encode negative dependencies**.
Turns out, there are a lot of additional nice properties about this, see [[1]](#1) and [[2]](#2) if you want to dig into the details. More about this in a bit.

Now what does this all have to do with mixtures, and why are you telling me about this... Just a sec.

### Mixtures with Negative Weights
OK, so far we have talked about vanilla mixtures, and some fancy extensions of linear models. 
Now, if we look up literature about kernel functions, one that pops up all the time is the Gaussian/RBF/Squared Exponential (SE) kernel:
$$
k_{\text{SE}}(x,x'; \sigma) = \exp\left(-\frac{(x-x')^2}{2\sigma^2}\right)
$$
which we can see as the density function of an unnormalized Gaussian distribution, i.e., 
$$
\mathcal{N}(x \mid \mu, \sigma^2) \propto k_{\text{SE}}(x,\mu; \sigma).
$$
Now, if we do this it becomes obvious that Eq. (3) can be seen as a quadratic form of a mixture of unnormalized Gaussians if we assume an SE kernel function.
Observe that, if $\textbf{A}$ is a diagonal matrix then we retain a simple mixture of unnormalized Gaussians which we can re-normalize to obtain a proper probabilistic model, i.e., 
$$
p(x) = \frac{1}{z}\sum^k_{i,j=1} A_{i,j} k(x, \mu_i; 2\sigma_i) k(x, \mu_j; 2\sigma_j) = \frac{1}{z} \sum^k_{i=1} A_{i,i} k(x, x_j; 2\sigma)^2 = \frac{1}{z} \sum^k_{i=1} A_{i,i} k(x, x_j; \sigma)
$$
where \\(z = \int_x \sum^k_{i=1} A_{i,i} k(x, x_j; \sigma) dx\\) is the normalization constant of the model. Note that we can compute \\(z\\) for certain choices of $k$ in closed form.

If $\textbf{A}$ is not a diagonal matrix, then we can have **negative** weights in our mixture model at the off-diagonal elements. Meaning, we can encode a negative dependence between mixing components.
In fact, the resulting model has many interesting properties, such as:
1. Assuming SE kernel functions, the family of models expressed through Eq. (3) is a universal approximator for probabilities that admit a density.
2. The number of SE components needed to approximate a d-dimensional and $\beta$-times differentiable density function with precision \\(\epsilon\\) corresponds to the optimal rate for function interpolation and to models with negative weights, i.e., $k=\epsilon^{-d/\beta}$.

For more details and additional theoretic results on this family of models see [[2]](#2).

#### So what does that mean and what did we gain?
The easiest way to see what we have gained through this is to visualize the functions we can represent.
The visualization below shows a mixture of two Gaussian components and negative interactions between them.
Note that I did not care to normalize the output function, but that is easily done if necessary.
The mixture itself is specified by the following parameters
$$
\textbf{A} = \left[\\begin{array}{cc}
2 & -3.1\\\\
-3.1 & 5
\\end{array}\right], \mu_1 = \mu_2 = \left[\\begin{array}{c}
0\\\\
0
\\end{array}\right], \sigma_1 = 0.2,  \sigma_2 = 10 .
$$
At the end of this post you can find a ~5 lines implementation of these models and code to reproduce the center plot.

{{< load-plotly >}}
{{< plotly json="../plotly/psd.json" height="600px" width="600px" modebar="false">}}

We see that using the quadratic form in Eq. (3) to represent a mixture with negative weights allows us to model a fairly complicated density function with only two mixing components. Note that modeling the same density function with a mixture of positive weights requires covering the ring with multiple components as the model can only additively combine density functions and cancelation effects cannot occur.

### Generalization to (Shallow) Neural Networks
A recent paper [[4]](#4) introduced another twist to the story, by considering the squared 2-norm of shallow (single hidden layer) neural networks. Those can be seen as yet another instance of Eq. (3) but have the nice property of abstracting the formulation to a modeling family many people are familiar with.
The resulting family of models is coined, squared neural families. I will not go into details about these models here, and possibly write a future post about them instead.
However, the interesting aspects of the work by [[4]](#4) are:
1. The abstraction of the model in Eq. (3) to shallow neural networks with (non)-linear activation functions
2. Their study of conditions under which these models can still be marginalized; a condition needed to represent density functions of a probabilistic model. In short, we need to have a tractable representation of the product of activation functions/kernel functions that allows closed-form integration. An easy way to achieve this is to resort to exponential or linear functions or use sinusoidal activation functions, like those that we investigated in the context of Bayesian neural networks [[5]](#5), or combinations thereof.

## Conclusion
Congrats, you managed to read through everything I wrote or maybe you just scrolled down. 
In any case, I hope this was informative and maybe made you similarly curious as I am about this new family of models.
Many of the papers around these models are rather technical and theoretical in nature, and it will be interesting to see how well they perform in application scenarios in the future and what computational challenges are associated with these models.

### Thanks
Thanks to Ti John for valuable comments.

</br>

<details>
<summary>Julia code</summary>

```Julia
using PlotlyJS, LinearAlgebra

# PSD Models
_k(z::AbstractVector, η::AbstractVector) = exp( -z' * Diagonal(1 ./ η) * z )
k(x, μ, η) = _k(x-μ, η)
f(k::AbstractVector, A::AbstractMatrix) = k' * A * k 
f(x::AbstractVector, μ, η, A::AbstractMatrix) = f(k.(Ref(x), μ, η), A)

# Some fancy PSD matrix
A = Symmetric([2 -3.1; -3.1 5]) 

# locations
μ = map(k -> zeros(2), 1:2)

# scales
η = [ones(2)*0.1, ones(2)*5] * 2

# evaluation grid points
xs = range(-4, 4, length=500)
ys = range(-4, 4, length=500)

# compute function outputs
z = [f([x,y], μ, η, A) for x in xs, y in ys]

# plotting
plot(heatmap(x=xs, y=ys, z=z, reversescale = false, colorscale = "Blues", zsmooth="best"))
```

</details>

--- 



## References
<a id="1">[1]</a> Ulysse Marteau-Ferey, Francis R. Bach, and Alessandro Rudi.
*Non-parametric Models for Non-negative Functions.* NeurIPS 2020.
</br>
<a id="2">[2]</a> Alessandro Rudi and Carlo Ciliberto.
*PSD Representations for Effective Probability Models.* 
NeurIPS 2021.
</br>
<a id="3">[3]</a> Bernhard Schölkopf, Ralf Herbrich, and Alex J. Smola. 
*A Generalized Representer Theorem.*
COLT 2001.
</br>
<a id="4">[4]</a> Russel Tsuchida, Cheng Soon Ong, and Dino Sejdinovic.
*Squared Neural Families: A new class of tractable density models.*
ArXiv, 2023. 
</br>
<a id="5">[5]</a> Lassi Meronen, Martin Trapp, Arno Solin,
*Periodic Activation Functions induce Stationarity.*
NeurIPS 2021. 
</br>


